{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For splitting the csv in one txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):#+\n",
    "\n",
    "# Load the CSV file with error handling using on_bad_lines\n",
    "csv_file_path = 'SNET_Digitization - 1-100.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path, encoding_errors='ignore', on_bad_lines='skip')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error reading the CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Specify the columns to extract\n",
    "document_id_column = 'Serial Number'\n",
    "article_text_column = 'Article Text'\n",
    "\n",
    "# Function to clean up non-printable characters\n",
    "def clean_text(text):\n",
    "    # Replace non-printable characters with an empty string\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', str(text))\n",
    "\n",
    "# Apply cleaning function to 'Article Text' column\n",
    "df[article_text_column] = df[article_text_column].apply(clean_text)\n",
    "\n",
    "# Initialize an output file\n",
    "output_file_path = 'output_climate_finance_serialno_SNETs_1-100.txt'\n",
    "\n",
    "# Group by 'Document ID' and write the output\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for document_id, group in df.groupby(document_id_column):\n",
    "        output_file.write(f'Serial_Number: {document_id}\\n')\n",
    "        # Merge all text for that Document ID and write it\n",
    "        merged_text = ' '.join(group[article_text_column].dropna().astype(str))\n",
    "        output_file.write(f'{merged_text}\\n\\n')\n",
    "\n",
    "print(f\"Output written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For splitting  one txt file into multiple smaller chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_file(file_path, encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def split_content_by_serial_number(content):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_word_count = 0\n",
    "    serial_number = None\n",
    "    \n",
    "    for line in content:\n",
    "        if \"Serial_Number: \" in line:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [line]\n",
    "                current_chunk_word_count = len(line.split())\n",
    "                serial_number = line.split()[1]\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                serial_number = line.split()[1]\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_chunk_word_count += len(line.split())\n",
    "        \n",
    "        if current_chunk_word_count > 2000:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_chunk_word_count = 0\n",
    "            if \"Serial_Number: \" in line:\n",
    "                serial_number = line.split()[1]\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def save_chunks_to_files(chunks, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join(output_folder, f'chunk_{i + 1}.txt')\n",
    "        with open(chunk_filename, 'w', encoding='utf-8') as chunk_file:\n",
    "            for line in chunk:\n",
    "                chunk_file.write(line)\n",
    "\n",
    "def process_large_text_file(input_file, output_folder):\n",
    "    content = read_file(input_file)\n",
    "        \n",
    "    # Split the content into chunks of 2000 words each\n",
    "    chunks = split_content_by_serial_number(content)\n",
    "        \n",
    "    # Save the chunks to separate files\n",
    "    save_chunks_to_files(chunks, output_folder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'output_climate_finance_serialno_SNETs_1-100.txt'\n",
    "    output_folder = 'output_chunks_folder_climate_finance_serialno_SNETs_1-100'\n",
    "    process_large_text_file(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import faiss\n",
    "from typing import Optional\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# Create the Bedrock client\n",
    "BEDROCK_CLIENT = boto3.client(\n",
    "    \"bedrock\",\n",
    "    region_name='us-east-1',\n",
    "    config=config,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "class BaseConversation(ABC):\n",
    "\n",
    "    @abstractproperty\n",
    "    def default_model(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def embeddings(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_conversation_chain(\n",
    "        self,\n",
    "        store: FAISS\n",
    "    ) -> BaseConversationalRetrievalChain:\n",
    "        pass\n",
    "\n",
    "    def create_store(self, texts: list):\n",
    "        \"\"\"Create a vector store from the provided texts.\"\"\"\n",
    "        store: FAISS = FAISS.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        faiss.write_index(store.index, \"docs.index\")\n",
    "        store.index = None\n",
    "        with open(\"faiss_store.pkl\", \"wb\") as f:\n",
    "            pickle.dump(store, f)\n",
    "\n",
    "    def get_chain(self) -> BaseConversationalRetrievalChain:\n",
    "        \"\"\"Create a conversation chain from the stored vector store.\"\"\"\n",
    "        if not os.path.exists(\"docs.index\"):\n",
    "            raise FileNotFoundError(\"No vector store found.\")\n",
    "        if not os.path.exists(\"faiss_store.pkl\"):\n",
    "            raise FileNotFoundError(\"No vector store found.\")\n",
    "\n",
    "        index = faiss.read_index(\"docs.index\")\n",
    "        with open(\"faiss_store.pkl\", \"rb\") as f:\n",
    "            store = pickle.load(f)\n",
    "\n",
    "        store.index = index\n",
    "        return self.get_conversation_chain(store=store)\n",
    "\n",
    "class HFConversation(BaseConversation):\n",
    "    def __init__(self, model_name: Optional[str] = None) -> None:\n",
    "        self.model_name = model_name\n",
    "        self._embeddings: Optional[Embeddings] = None\n",
    "\n",
    "    @property\n",
    "    def default_model(self) -> str:\n",
    "        return 'hkunlp/instructor-large'\n",
    "\n",
    "    @property\n",
    "    def embeddings(self) -> Embeddings:\n",
    "        if self._embeddings is None:\n",
    "            self._embeddings = HuggingFaceEmbeddings()\n",
    "        return self._embeddings\n",
    "\n",
    "    def get_conversation_chain(self, store: FAISS) -> BaseConversationalRetrievalChain:\n",
    "        \"\"\"Create a conversation chain from the provided vector store.\"\"\"\n",
    "        llm = BedrockChat(model_id=\"mistral.mistral-large-2402-v1:0\", region_name='us-east-1', model_kwargs={\"temperature\": 0.3,\"max_tokens\": 4000})\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        return ConversationalRetrievalChain.from_llm(llm=llm, retriever=store.as_retriever(), memory=memory)\n",
    "\n",
    "def extract_information_from_chunk(chunk, conversation_chain):\n",
    "    prompt = f\"\"\"You are an AI assistant tasked with identifying articles that mention data usage in disaster contexts. Your goal is to determine whether a given article discusses the collection, analysis, or application of data in relation to disasters. Follow these instructions carefully:\n",
    "\n",
    "1. Understanding Data in Disasters:\n",
    "\"Data in Disasters\" refers to the various types of information collected, analyzed, and utilized before, during, and after disaster events. Key aspects include:\n",
    "\n",
    "a) Types of Data:\n",
    "- Hazard Data (e.g., frequency, intensity, location of disasters)\n",
    "- Exposure Data (populations and assets in disaster-prone areas)\n",
    "- Vulnerability Data (social, economic, environmental factors)\n",
    "- Impact Data (casualties, economic losses, damage assessments)\n",
    "- Response and Recovery Data (relief operations, recovery efforts)\n",
    "\n",
    "b) Data Sources:\n",
    "- Remote Sensing (satellite images, aerial photos, GIS data)\n",
    "- Crowdsourced Data (social media, mobile apps, local reporting)\n",
    "- Field Surveys (ground assessments by response teams)\n",
    "- Statistical Data (censuses, surveys, economic reports)\n",
    "\n",
    "c) Indicators and Metrics:\n",
    "- Mortality rates\n",
    "- Economic losses\n",
    "- Affected population numbers\n",
    "- Disaster risk indexes\n",
    "\n",
    "d) Data Applications:\n",
    "- Early Warning Systems\n",
    "- Big Data and AI in disaster response\n",
    "- Humanitarian data management\n",
    "- Climate and environmental monitoring\n",
    "- Geospatial mapping and analysis\n",
    "\n",
    "e) Open Data Repositories:\n",
    "- UN OCHA's Humanitarian Data Exchange (HDX)\n",
    "- World Bank Open Data\n",
    "- EM-DAT (Emergency Events Database)\n",
    "- NASA Earth Data\n",
    "\n",
    "2. Article Analysis:\n",
    "Carefully read the provided article text:\n",
    "\n",
    "<article>\n",
    "{{ARTICLE_TEXT}}\n",
    "</article>\n",
    "\n",
    "3. As you read, consider the following questions:\n",
    "- Does the article mention any specific types of data used in disaster contexts?\n",
    "- Are there examples of data collection, analysis, or application in disaster management?\n",
    "- Does the content discuss the role of data in disaster prevention, preparedness, response, or recovery?\n",
    "- Are there mentions of data sources, indicators, or metrics related to disasters?\n",
    "- Does the article describe any challenges or innovations in disaster-related data usage?\n",
    "- Is there discussion of open data repositories or data sharing in disaster contexts?\n",
    "\n",
    "4. Make a Decision and Provide Output:\n",
    "Based on your analysis, determine whether the article mentions data usage in disaster contexts.\n",
    "\n",
    "\n",
    "If the article is relevant, output the following:\n",
    "\n",
    "YOUT OUTPUT MUST BE IN THE BELOW FORMAT\n",
    "\n",
    "Serial Number : 1\n",
    "Decision : Yes\n",
    "\n",
    "Serial Number : 2\n",
    "Decision : No\n",
    "\n",
    "STRICTLY dont give any other filling text or reason for your decision ,output must only contain \"[\"Yes\",\"<Document ID>\" \"<Serial_Number>\"]\"\n",
    "\n",
    "ALL ARTICLES MUST BE COVERED, NOT ONE ARTICLE SHOULD BE ABSENT IN THE OUTPUT.\n",
    "\n",
    "\"\"\"\n",
    "    full_input = f\"{prompt}\\n\\n{chunk}\"\n",
    "    # Use the appropriate method to run the full input as a single string\n",
    "    response = conversation_chain.run(full_input)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    # Initialize conversation\n",
    "    conversation = HFConversation()\n",
    "\n",
    "    # Check if vector store exists, if not create it\n",
    "    if not os.path.exists(\"docs.index\") or not os.path.exists(\"faiss_store.pkl\"):\n",
    "        # Assuming you have the texts to create the store\n",
    "        texts = [\"Your initial texts for creating the FAISS store go here\"]\n",
    "        conversation.create_store(texts)\n",
    "\n",
    "    # Folder containing file chunks\n",
    "    folder_path = 'file_chunks'\n",
    "    output_file = 'output.txt'\n",
    "\n",
    "    processed_serial_numbers = set()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                        chunk = infile.read()\n",
    "\n",
    "                        # Reinitialize conversation chain for each chunk to clear context\n",
    "                        conversation_chain = conversation.get_chain()\n",
    "\n",
    "                        response = extract_information_from_chunk(chunk, conversation_chain)\n",
    "\n",
    "                        # Extract the serial number from the chunk\n",
    "                        if 'Serial_Number: ' in chunk:\n",
    "                            serial_number = chunk.split('Serial_Number: ')[1].split()[0]\n",
    "                        else:\n",
    "                            serial_number = \"Not Mentioned\"\n",
    "\n",
    "                        # Write the serial number only if it hasn't been processed\n",
    "                        if serial_number not in processed_serial_numbers:\n",
    "                            outfile.write(f\"Serial_Number: {serial_number:}\\n\")\n",
    "                            processed_serial_numbers.add(serial_number)\n",
    "\n",
    "                        # Write the response\n",
    "                        for line in response.split('\\n'):\n",
    "                            if line.strip() and \"Serial_Number: \" not in line:  # Check if line is not empty and does not contain \"Serial Number\"\n",
    "                                outfile.write(line.strip() + \"\\n\")\n",
    "\n",
    "                        outfile.write(\"-\" * 40 + \"\\n\")  # Add separator line\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Error reading {file_path}. Skipping this file due to encoding issues.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting Output from LLM (txt) into csv with Serial Number and Decision as two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to process each line and extract Serial Number and Decision\n",
    "def extract_data_from_line(line):\n",
    "    serial_number = None\n",
    "    decision = None\n",
    "\n",
    "    # Check for the first format\n",
    "    if \"Serial Number\" in line:\n",
    "        match = re.search(r\"Serial Number\\s*:\\s*(\\d+|Not Mentioned)\", line)\n",
    "        if match:\n",
    "            serial_number = match.group(1)\n",
    "    elif \"Decision\" in line:\n",
    "        match = re.search(r\"Decision\\s*:\\s*(Yes|No)\", line)\n",
    "        if match:\n",
    "            decision = match.group(1)\n",
    "\n",
    "    # Check for the second format\n",
    "    elif \"<output>\" in line:\n",
    "        match = re.search(r'\\[\"(Yes|No)\",,?\"(\\d+)\"\\]', line)\n",
    "        if match:\n",
    "            decision = match.group(1)\n",
    "            serial_number = match.group(2)\n",
    "\n",
    "    return serial_number, decision\n",
    "\n",
    "# Function to convert text file to CSV\n",
    "def convert_txt_to_csv(txt_file_path, csv_file_path):\n",
    "    serial_number = None\n",
    "    decision = None\n",
    "    processed_serial_numbers = set()  # Set to track already processed serial numbers\n",
    "\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as txt_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"Serial Number\", \"Decision\"])\n",
    "\n",
    "        for line in txt_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                sn, dec = extract_data_from_line(line)\n",
    "                \n",
    "                if sn is not None:\n",
    "                    serial_number = sn\n",
    "                if dec is not None:\n",
    "                    decision = dec\n",
    "\n",
    "                # Write to CSV when both Serial Number and Decision are found\n",
    "                if serial_number and decision:\n",
    "                    if serial_number not in processed_serial_numbers:\n",
    "                        csv_writer.writerow([serial_number, decision])\n",
    "                        processed_serial_numbers.add(serial_number)  # Mark this serial number as processed\n",
    "                    serial_number = None\n",
    "                    decision = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_txt_file = 'output.txt'  # Replace with your text file path\n",
    "    output_csv_file = 'SNETs_half_5.csv'  # Replace with your desired CSV file path\n",
    "    convert_txt_to_csv(input_txt_file, output_csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
