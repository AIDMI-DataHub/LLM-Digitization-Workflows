import os
import pickle
from abc import ABC, abstractmethod, abstractproperty
from dotenv import load_dotenv
import boto3
import faiss
from typing import Optional
from langchain.llms.bedrock import Bedrock
from langchain_community.chat_models import BedrockChat
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings.base import Embeddings
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS

from boto3 import client
from botocore.config import Config

config = Config(read_timeout=1000)

# Load environment variables from .env file
load_dotenv()

# Access the environment variables
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')

# Create the Bedrock client
BEDROCK_CLIENT = boto3.client(
    "bedrock",
    region_name='us-east-1',
    config=config,
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)

class BaseConversation(ABC):

    @abstractproperty
    def default_model(self) -> str:
        pass

    @abstractproperty
    def embeddings(self):
        pass

    @abstractmethod
    def get_conversation_chain(
        self,
        store: FAISS
    ) -> BaseConversationalRetrievalChain:
        pass

    def create_store(self, texts: list):
        """Create a vector store from the provided texts."""
        store: FAISS = FAISS.from_texts(
            texts=texts,
            embedding=self.embeddings
        )
        faiss.write_index(store.index, "docs.index")
        store.index = None
        with open("faiss_store.pkl", "wb") as f:
            pickle.dump(store, f)

    def get_chain(self) -> BaseConversationalRetrievalChain:
        """Create a conversation chain from the stored vector store."""
        if not os.path.exists("docs.index"):
            raise FileNotFoundError("No vector store found.")
        if not os.path.exists("faiss_store.pkl"):
            raise FileNotFoundError("No vector store found.")

        index = faiss.read_index("docs.index")
        with open("faiss_store.pkl", "rb") as f:
            store = pickle.load(f)

        store.index = index
        return self.get_conversation_chain(store=store)

class HFConversation(BaseConversation):
    def __init__(self, model_name: Optional[str] = None) -> None:
        self.model_name = model_name
        self._embeddings: Optional[Embeddings] = None

    @property
    def default_model(self) -> str:
        return 'hkunlp/instructor-large'

    @property
    def embeddings(self) -> Embeddings:
        if self._embeddings is None:
            self._embeddings = HuggingFaceEmbeddings()
        return self._embeddings

    def get_conversation_chain(self, store: FAISS) -> BaseConversationalRetrievalChain:
        """Create a conversation chain from the provided vector store."""
        llm = BedrockChat(model_id="mistral.mistral-large-2402-v1:0", region_name='us-east-1', model_kwargs={"temperature": 0.3,"max_tokens": 4000})
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        return ConversationalRetrievalChain.from_llm(llm=llm, retriever=store.as_retriever(), memory=memory)

def extract_information_from_chunk(chunk, conversation_chain):
    prompt = f"""Given a text that provides context place and year for a particular event , categorize each one of those pairs of context, place and year into two of the following categories based on its content, because its hard to tag them into only one. The categories are: 1. Description of Disaster Event: Describes a disaster event. 2. Description of Climate Event: Describes an extreme climate event. 3. Definition of Disaster/Climate Event: Defines a disaster or climate event. 4. Losses and Damages: Describes the extent of loss and damage caused by the event. 5. Definition of Vulnerability: Defines any vulnerability within the text. 6. AIDMI Action/Recommendation/Intervention: Describes actions, recommendations, or interventions by AIDMI. 7. Other Organization Action/Recommendation/Intervention: Describes actions, recommendations, or interventions by other organizations. 8. Govt. Action/Recommendation/Intervention: Describes actions, recommendations, or interventions by the government. 9. Concept related to Disaster Risk Reduction/Humanitarian Action/Climate Change Adaptation: References any DRR, HA, or CCA concept or solution. For each context, provide the category it belongs. Format should be Place: Year: Context: Category: .No need to insert serial numbers. the format should be like "Place: Year: Context: Category:  ",Strictly dont use any filler AI text,just keep the Place,Year and context as it is and below that context insert the category it belongs to, and remember if the context is about people affected or disrupted by any event then it should come under 5. Definition of Vulnerability and if there is clearly mentioned that people died or damaged is caused then it comes under losses and damages.,this is how the format must be
    Place: New Orleans
    Year: 2005
    Context: Hurricane Katrina struck the city, causing widespread flooding and devastation.
    Primary Category: Description of Disaster Event
    Secondary Category:  Definition of Vulnerability

    Place: Paris
    Year: 2015
    Context: The Paris Agreement was adopted, aiming to limit global warming to well below 2 degrees Celsius.
    Primary Category: Concept related to Disaster Risk Reduction/Humanitarian Action/Climate Change Adaptation
    Secondary Category:  Description of Climate Event"""
    full_input = f"{prompt}\n\n{chunk}"
    # Use the appropriate method to run the full input as a single string
    response = conversation_chain.run(full_input)
    return response

def main():
    # Initialize conversation
    conversation = HFConversation()

    # Check if vector store exists, if not create it
    if not os.path.exists("docs.index") or not os.path.exists("faiss_store.pkl"):
        # Assuming you have the texts to create the store
        texts = ["Your initial texts for creating the FAISS store go here"]
        conversation.create_store(texts)

    # Folder containing file chunks
    folder_path = 'file_chunks'
    output_file = 'output.txt'

    processed_serial_numbers = set()

    with open(output_file, 'w', encoding='utf-8') as outfile:
        for filename in os.listdir(folder_path):
            if filename.endswith('.txt'):
                file_path = os.path.join(folder_path, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        chunk = infile.read()

                        # Reinitialize conversation chain for each chunk to clear context
                        conversation_chain = conversation.get_chain()

                        response = extract_information_from_chunk(chunk, conversation_chain)

                        # Extract the serial number from the chunk,because Serial_Number is not extracted throug prompt but through this code
                        if 'Serial_Number:' in chunk:
                            serial_number = chunk.split('Serial_Number:')[1].split()[0]
                        else:
                            serial_number = "Not Mentioned"

                        # Write the serial number only if it hasn't been processed
                        if serial_number not in processed_serial_numbers:
                            outfile.write(f"Serial_Number: {serial_number}\n")
                            processed_serial_numbers.add(serial_number)

                        # Write the response
                        for line in response.split('\n'):
                            if line.strip() and "Serial_Number:" not in line:  # Check if line is not empty and does not contain "Serial Number"
                                outfile.write(line.strip() + "\n")

                        outfile.write("-" * 40 + "\n")  # Add separator line
                except UnicodeDecodeError:
                    print(f"Error reading {file_path}. Skipping this file due to encoding issues.")

if __name__ == '__main__':
    main()
